{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create sparksession object\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('handling_time').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import Linear Regression from spark's MLlib\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the dataset\n",
    "df=spark.read.csv('/FileStore/tables/data_handling.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(670857, 14)\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validate the size of data\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- _c0: integer (nullable = true)\n-- SHP_DATE_CREATED_ID: timestamp (nullable = true)\n-- SHP_DATETIME_CREATED_ID: timestamp (nullable = true)\n-- SHP_DATE_HANDLING_ID: timestamp (nullable = true)\n-- SHP_DATETIME_HANDLING_ID: timestamp (nullable = true)\n-- SHP_STATUS_ID: string (nullable = true)\n-- SHP_SENDER_ID: string (nullable = true)\n-- SHP_ORDER_COST: double (nullable = true)\n-- CAT_CATEG_ID_L7: string (nullable = true)\n-- SHP_ADD_ZIP_CODE: integer (nullable = true)\n-- SHP_DATE_SHIPPED_ID: timestamp (nullable = true)\n-- SHP_DATETIME_SHIPPED_ID: timestamp (nullable = true)\n-- SHP_PICKING_TYPE_ID: string (nullable = true)\n-- HT_REAL: integer (nullable = true)\n\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#explore the data\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-3516167528173103&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> <span class=\"ansired\">#view statistical measures of data</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 2</span><span class=\"ansiyellow\"> </span>df<span class=\"ansiyellow\">.</span>describe<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>show<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">5</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansicyan\">describe</span><span class=\"ansiblue\">(self, *cols)</span>\n<span class=\"ansigreen\">   1195</span>         <span class=\"ansigreen\">if</span> len<span class=\"ansiyellow\">(</span>cols<span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">==</span> <span class=\"ansicyan\">1</span> <span class=\"ansigreen\">and</span> isinstance<span class=\"ansiyellow\">(</span>cols<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> list<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1196</span>             cols <span class=\"ansiyellow\">=</span> cols<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 1197</span><span class=\"ansiyellow\">         </span>jdf <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jdf<span class=\"ansiyellow\">.</span>describe<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jseq<span class=\"ansiyellow\">(</span>cols<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1198</span>         <span class=\"ansigreen\">return</span> DataFrame<span class=\"ansiyellow\">(</span>jdf<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>sql_ctx<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1199</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1255</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1256</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1257</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1258</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1259</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    327</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 328</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    329</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    330</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling o1374.describe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 10.0 failed 1 times, most recent failure: Lost task 4.0 in stage 10.0 (TID 35, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 403, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 398, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 365, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 147, in dump_stream\n    for obj in iterator:\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 354, in _batched\n    for item in iterator:\n  File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 85, in &lt;lambda&gt;\n    return lambda *a: f(*a)\n  File &quot;/databricks/spark/python/pyspark/util.py&quot;, line 99, in wrapper\n    return f(*args, **kwargs)\n  File &quot;&lt;command-1940436074516061&gt;&quot;, line 11, in shp_day\nTypeError: strptime() argument 1 must be str, not datetime.datetime\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:490)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:444)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:638)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:98)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:95)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:839)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:839)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:304)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:304)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:139)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1481)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:503)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2355)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2343)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2342)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2342)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1096)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1096)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1096)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2574)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2510)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:893)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2240)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2262)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2281)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2306)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:961)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:960)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.aggResult$lzycompute$1(StatFunctions.scala:273)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.org$apache$spark$sql$execution$stat$StatFunctions$$aggResult$1(StatFunctions.scala:273)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$summary$2.apply$mcVI$sp(StatFunctions.scala:286)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.summary(StatFunctions.scala:285)\n\tat org.apache.spark.sql.Dataset.summary(Dataset.scala:2546)\n\tat org.apache.spark.sql.Dataset.describe(Dataset.scala:2485)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 403, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 398, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 365, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 147, in dump_stream\n    for obj in iterator:\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 354, in _batched\n    for item in iterator:\n  File &quot;&lt;string&gt;&quot;, line 1, in &lt;lambda&gt;\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 85, in &lt;lambda&gt;\n    return lambda *a: f(*a)\n  File &quot;/databricks/spark/python/pyspark/util.py&quot;, line 99, in wrapper\n    return f(*args, **kwargs)\n  File &quot;&lt;command-1940436074516061&gt;&quot;, line 11, in shp_day\nTypeError: strptime() argument 1 must be str, not datetime.datetime\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:490)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:444)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:638)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:98)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:95)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:839)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:839)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:304)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:340)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:304)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:139)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1481)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:503)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view statistical measures of data \n",
    "df.describe().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">7</span><span class=\"ansired\">]: </span>\n[Row(_c0=533546, SHP_DATE_CREATED_ID=datetime.datetime(2017, 7, 28, 0, 0), SHP_DATETIME_CREATED_ID=datetime.datetime(2017, 7, 28, 8, 44, 1), SHP_DATE_HANDLING_ID=datetime.datetime(2017, 7, 28, 0, 0), SHP_DATETIME_HANDLING_ID=datetime.datetime(2017, 7, 28, 8, 44, 13), SHP_STATUS_ID=&apos;delivered&apos;, SHP_SENDER_ID=&apos;85ebf03e2c86c1c992a4114e4eaf6bf9&apos;, SHP_ORDER_COST=200.0, CAT_CATEG_ID_L7=&apos;412cf38b165cf0c07760390641f8aa35&apos;, SHP_ADD_ZIP_CODE=1623, SHP_DATE_SHIPPED_ID=datetime.datetime(2017, 8, 1, 0, 0), SHP_DATETIME_SHIPPED_ID=datetime.datetime(2017, 8, 1, 11, 8, 55), SHP_PICKING_TYPE_ID=&apos;drop_off&apos;, HT_REAL=50),\n Row(_c0=934098, SHP_DATE_CREATED_ID=datetime.datetime(2017, 7, 2, 0, 0), SHP_DATETIME_CREATED_ID=datetime.datetime(2017, 7, 2, 17, 9, 7), SHP_DATE_HANDLING_ID=datetime.datetime(2017, 7, 3, 0, 0), SHP_DATETIME_HANDLING_ID=datetime.datetime(2017, 7, 3, 13, 11, 59), SHP_STATUS_ID=&apos;delivered&apos;, SHP_SENDER_ID=&apos;ced3662210bef7ff33dc58f87ef49687&apos;, SHP_ORDER_COST=1151.0, CAT_CATEG_ID_L7=&apos;bf01dfb57c99031fbb0b856684643dc9&apos;, SHP_ADD_ZIP_CODE=1631, SHP_DATE_SHIPPED_ID=datetime.datetime(2017, 7, 4, 0, 0), SHP_DATETIME_SHIPPED_ID=datetime.datetime(2017, 7, 4, 19, 2, 38), SHP_PICKING_TYPE_ID=&apos;drop_off&apos;, HT_REAL=30),\n Row(_c0=998176, SHP_DATE_CREATED_ID=datetime.datetime(2017, 8, 17, 0, 0), SHP_DATETIME_CREATED_ID=datetime.datetime(2017, 8, 17, 11, 1, 34), SHP_DATE_HANDLING_ID=datetime.datetime(2017, 8, 17, 0, 0), SHP_DATETIME_HANDLING_ID=datetime.datetime(2017, 8, 17, 11, 1, 44), SHP_STATUS_ID=&apos;delivered&apos;, SHP_SENDER_ID=&apos;5fbbc02fe31c3f7668da5f38bd756911&apos;, SHP_ORDER_COST=359.88, CAT_CATEG_ID_L7=&apos;adce58cb62e031fa3e81ee43507975e0&apos;, SHP_ADD_ZIP_CODE=1425, SHP_DATE_SHIPPED_ID=datetime.datetime(2017, 8, 17, 0, 0), SHP_DATETIME_SHIPPED_ID=datetime.datetime(2017, 8, 17, 16, 39, 4), SHP_PICKING_TYPE_ID=&apos;drop_off&apos;, HT_REAL=6)]\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sneak into the dataset\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import udf, array\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# 1. SHP_ORDER_COST_INT: Se tranforma la columna SHP_ORDER_COST de float a integer.\n",
    "df = df.withColumn(\"SHP_ORDER_COST_INT\",(df[\"SHP_ORDER_COST\"].cast(IntegerType())))\n",
    "\n",
    "# 2. SHP_STATUS_ID_ENC: Se realiza el encoding de la SHP_STATUS_ID para ser representada como integer.\n",
    "# 3. SHP_PICKING_TYPE_ID_ENC: Se realiza el encoding de la SHP_STATUS_ID para ser representada como integer.\n",
    "# 4. SHP_DAY: Se añade una columna para indicar que dia de la semana se acredito el pago.\n",
    "\n",
    "def shp_day(x):\n",
    "    return x.weekday()\n",
    "  \n",
    "shp_day_udf= udf(shp_day, IntegerType())\n",
    "\n",
    "df = df.withColumn('SHP_DAY', shp_day_udf(df['SHP_DATE_HANDLING_ID']))\n",
    "\n",
    "# 5. WKND_DAY: Se añade una columna para indicar si el pago se acredito durante el fin de semana.\n",
    "def weekend_day(x):\n",
    "    return 0 if x.weekday() < 5 else 1\n",
    "  \n",
    "weekend_day_udf= udf(weekend_day, IntegerType())\n",
    "\n",
    "df = df.withColumn('WKND_DAY', weekend_day_udf(df['SHP_DATE_HANDLING_ID']))\n",
    "df.select('WKND_DAY').show(10)\n",
    "\n",
    "# 6. MONTH_NUM: Se añanade una columna para indicar el mes del pago.\n",
    "def week_number(x):\n",
    "    return x.isocalendar()[1]\n",
    "  \n",
    "week_number_udf= udf(week_number, IntegerType())\n",
    "\n",
    "df = df.withColumn('WK_NUM', week_number_udf(df['SHP_DATE_HANDLING_ID']))\n",
    "df.select('WK_NUM').show(10)\n",
    "\n",
    "# 7. *WK_NUM*: Se añanade una columna para indicar la semana del año en la que se realizó el pago.\n",
    "def month_number(x):\n",
    "    return x.month\n",
    "\n",
    "month_number_udf= udf(month_number, IntegerType())\n",
    "  \n",
    "df = df.withColumn('MONTH_NUM', month_number_udf(df['SHP_DATE_HANDLING_ID']))\n",
    "\n",
    "# 8. *TIMESTAMP*: Se añanade un TIMESTAMP de las fechas.\n",
    "def get_timestamp(x):\n",
    "    return int(datetime.timestamp(x))\n",
    "\n",
    "get_timestamp_udf = udf(get_timestamp, IntegerType())\n",
    "  \n",
    "df = df.withColumn('SHP_DATE_HANDLING_TIMESTAMP', get_timestamp_udf(df['SHP_DATE_HANDLING_ID']))\n",
    "df = df.withColumn('SHP_DATE_CREATED_TIMESTAMP', get_timestamp_udf(df['SHP_DATE_CREATED_ID']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+\n HT|\n+---+\n 50|\n 29|\n  5|\n 40|\n 27|\n 73|\n 31|\n161|\n  5|\n114|\n+---+\nonly showing top 10 rows\n\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def horas_habiles(a, b):\n",
    "    \"\"\"\n",
    "    Retorna la diferencia en horas habiles entre dos fechas.\n",
    "    No se tienen en cuenta los fines de semana.\n",
    "    TODO: Anadir feriados para que no se tengan en cuenta.\n",
    "    \"\"\"\n",
    "    start_delta = 0\n",
    "    if a.weekday() < 5:\n",
    "        next_day = a + timedelta(days = 1)\n",
    "        next_day = next_day.replace(hour=0, minute=0, second=0)\n",
    "        start_delta = (next_day - a).total_seconds()       \n",
    "        start = a + timedelta(days = 1)\n",
    "        start = start.replace(hour=0, minute=0, second=0)\n",
    "        \n",
    "    elif a.weekday() == 5:\n",
    "        start = a + timedelta(days = 2)\n",
    "        start = start.replace(hour=0, minute=0, second=0)\n",
    "\n",
    "    else:\n",
    "        start = a + timedelta(days = 1)\n",
    "        start = start.replace(hour=0, minute=0, second=0)\n",
    "    \n",
    "    end = b.replace(hour=0, minute=0, second=0)\n",
    "    end_delta = (b - end).total_seconds()\n",
    "    \n",
    "    total = start_delta + end_delta\n",
    "    \n",
    "    target_day = start\n",
    "    while target_day <= end:\n",
    "        if target_day.weekday() < 5:\n",
    "            total += 86400\n",
    "        target_day = target_day + timedelta(days = 1)\n",
    "    total -= 24*3600\n",
    "    return int(round(abs(total//3600)))\n",
    "\n",
    "def my_handling_time(row):\n",
    "    b = row[0] \n",
    "    a = row[1]\n",
    "    return horas_habiles(a, b)\n",
    "  \n",
    "my_handling_time_udf = udf(my_handling_time, IntegerType())\n",
    "  \n",
    "df = df.withColumn('HT', my_handling_time_udf(array('SHP_DATETIME_SHIPPED_ID', 'SHP_DATETIME_HANDLING_ID')))\n",
    "df.select('HT').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+\nSHP_SENDER_ID_NUM|CAT_CATEG_ID_L7_NUM|\n+-----------------+-------------------+\n7164.0           |219.0              |\n11311.0          |1501.0             |\n1007.0           |3595.0             |\n+-----------------+-------------------+\nonly showing top 3 rows\n\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "shp_sender_indexer =StringIndexer(inputCol=\"SHP_SENDER_ID\", outputCol=\"SHP_SENDER_ID_NUM\").fit(df)\n",
    "df = shp_sender_indexer.transform(df)\n",
    "\n",
    "shp_sender_indexer =StringIndexer(inputCol=\"CAT_CATEG_ID_L7\", outputCol=\"CAT_CATEG_ID_L7_NUM\").fit(df)\n",
    "df = shp_sender_indexer.transform(df)\n",
    "\n",
    "df.select('SHP_SENDER_ID_NUM','CAT_CATEG_ID_L7_NUM').show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import vectorassembler to create dense vectors\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">50</span><span class=\"ansired\">]: </span>\n[&apos;_c0&apos;,\n &apos;SHP_DATE_CREATED_ID&apos;,\n &apos;SHP_DATETIME_CREATED_ID&apos;,\n &apos;SHP_DATE_HANDLING_ID&apos;,\n &apos;SHP_DATETIME_HANDLING_ID&apos;,\n &apos;SHP_STATUS_ID&apos;,\n &apos;SHP_SENDER_ID&apos;,\n &apos;SHP_ORDER_COST&apos;,\n &apos;CAT_CATEG_ID_L7&apos;,\n &apos;SHP_ADD_ZIP_CODE&apos;,\n &apos;SHP_DATE_SHIPPED_ID&apos;,\n &apos;SHP_DATETIME_SHIPPED_ID&apos;,\n &apos;SHP_PICKING_TYPE_ID&apos;,\n &apos;HT_REAL&apos;,\n &apos;SHP_ORDER_COST_INT&apos;,\n &apos;SHP_DAY&apos;,\n &apos;WKND_DAY&apos;,\n &apos;WK_NUM&apos;,\n &apos;MONTH_NUM&apos;,\n &apos;SHP_DATE_HANDLING_TIMESTAMP&apos;,\n &apos;SHP_DATE_CREATED_TIMESTAMP&apos;,\n &apos;HT&apos;,\n &apos;SHP_SENDER_ID_NUM&apos;,\n &apos;CAT_CATEG_ID_L7_NUM&apos;]\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select the columns to create input vector\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the vector assembler \n",
    "vec_assmebler=VectorAssembler(inputCols=['SHP_DATE_HANDLING_TIMESTAMP', 'SHP_DATE_CREATED_TIMESTAMP','SHP_SENDER_ID_NUM', 'CAT_CATEG_ID_L7_NUM', \n",
    "                    'SHP_ORDER_COST_INT', 'SHP_DAY', 'WKND_DAY', \n",
    "                    'WK_NUM', 'MONTH_NUM', 'SHP_ADD_ZIP_CODE'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transform the values\n",
    "features_df=vec_assmebler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- _c0: integer (nullable = true)\n-- SHP_DATE_CREATED_ID: timestamp (nullable = true)\n-- SHP_DATETIME_CREATED_ID: timestamp (nullable = true)\n-- SHP_DATE_HANDLING_ID: timestamp (nullable = true)\n-- SHP_DATETIME_HANDLING_ID: timestamp (nullable = true)\n-- SHP_STATUS_ID: string (nullable = true)\n-- SHP_SENDER_ID: string (nullable = true)\n-- SHP_ORDER_COST: double (nullable = true)\n-- CAT_CATEG_ID_L7: string (nullable = true)\n-- SHP_ADD_ZIP_CODE: integer (nullable = true)\n-- SHP_DATE_SHIPPED_ID: timestamp (nullable = true)\n-- SHP_DATETIME_SHIPPED_ID: timestamp (nullable = true)\n-- SHP_PICKING_TYPE_ID: string (nullable = true)\n-- HT_REAL: integer (nullable = true)\n-- SHP_ORDER_COST_INT: integer (nullable = true)\n-- SHP_DAY: integer (nullable = true)\n-- WKND_DAY: integer (nullable = true)\n-- WK_NUM: integer (nullable = true)\n-- MONTH_NUM: integer (nullable = true)\n-- SHP_DATE_HANDLING_TIMESTAMP: integer (nullable = true)\n-- SHP_DATE_CREATED_TIMESTAMP: integer (nullable = true)\n-- HT: integer (nullable = true)\n-- SHP_SENDER_ID_NUM: double (nullable = false)\n-- CAT_CATEG_ID_L7_NUM: double (nullable = false)\n-- features: vector (nullable = true)\n\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validate the presence of dense vectors \n",
    "features_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------------------------------------------------------------+\nfeatures                                                             |\n+---------------------------------------------------------------------+\n[1.5012E9,1.5012E9,7164.0,219.0,200.0,4.0,0.0,30.0,7.0,1623.0]       |\n[1.49904E9,1.4989536E9,11311.0,1501.0,1151.0,0.0,0.0,27.0,7.0,1631.0]|\n[1.502928E9,1.502928E9,1007.0,3595.0,359.0,3.0,0.0,33.0,8.0,1425.0]  |\n[1.5027552E9,1.5027552E9,3375.0,0.0,300.0,1.0,0.0,33.0,8.0,1416.0]   |\n[1.502928E9,1.502928E9,18407.0,730.0,1600.0,3.0,0.0,33.0,8.0,1195.0] |\n+---------------------------------------------------------------------+\nonly showing top 5 rows\n\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view the details of dense vector\n",
    "features_df.select('features').show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+--------------------+\n            features|      scaledFeatures|\n+--------------------+--------------------+\n[1.5012E9,1.5012E...|[0.99776042264844...|\n[1.49904E9,1.4989...|[0.99632479614103...|\n[1.502928E9,1.502...|[0.99890892385437...|\n[1.5027552E9,1.50...|[0.99879407373377...|\n[1.502928E9,1.502...|[0.99890892385437...|\n[1.5005952E9,1.50...|[0.99735844722636...|\n[1.5005952E9,1.50...|[0.99735844722636...|\n[1.499904E9,1.499...|[0.99689904674399...|\n[1.49904E9,1.4990...|[0.99632479614103...|\n[1.499904E9,1.499...|[0.99689904674399...|\n[1.5036192E9,1.50...|[0.99936832433674...|\n[1.5008544E9,1.50...|[0.99753072240725...|\n[1.5027552E9,1.50...|[0.99879407373377...|\n[1.5031008E9,1.50...|[0.99902377397496...|\n[1.502064E9,1.502...|[0.99833467325140...|\n[1.5022368E9,1.50...|[0.99844952337199...|\n[1.5037056E9,1.50...|[0.99942574939703...|\n[1.5040512E9,1.50...|[0.99965544963822...|\n[1.5027552E9,1.50...|[0.99879407373377...|\n[1.5018912E9,1.50...|[0.99821982313081...|\n+--------------------+--------------------+\nonly showing top 20 rows\n\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "\n",
    "scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MaxAbsScalerModel\n",
    "scalerModel = scaler.fit(features_df)\n",
    "\n",
    "# rescale each feature to range [-1, 1].\n",
    "scaledData = scalerModel.transform(features_df)\n",
    "\n",
    "scaledData.select(\"features\", \"scaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create data containing input features and output column\n",
    "model_df = scaledData.select('scaledFeatures','HT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+---+\n      scaledFeatures| HT|\n+--------------------+---+\n[0.99776042264844...| 50|\n[0.99632479614103...| 29|\n[0.99890892385437...|  5|\n[0.99879407373377...| 40|\n[0.99890892385437...| 27|\n+--------------------+---+\nonly showing top 5 rows\n\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(670857, 2)\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size of model df\n",
    "print((model_df.count(), len(model_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data - Train & Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split the data into 70/30 ratio for train test purpose\n",
    "train_df,test_df=model_df.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(536241, 2)\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print((train_df.count(), len(train_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(134616, 2)\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print((test_df.count(), len(test_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----------------+\nsummary|               HT|\n+-------+-----------------+\n  count|           536241|\n   mean|31.19356781745521|\n stddev|37.20479546056911|\n    min|                0|\n    max|              913|\n+-------+-----------------+\n\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build Linear Regression model \n",
    "lin_Reg=LinearRegression(featuresCol=\"scaledFeatures\", labelCol='HT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the linear regression model on training data set \n",
    "lr_model=lin_Reg.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">84</span><span class=\"ansired\">]: </span>-206.77497404947576\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[9790.20703802,-9505.14316998,25.8321588788,-11.2850955237,16.7322181566,-4.29593269219,3.29062981445,-50.189214724,-7.21197545139,4.63113466896]\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lr_model.coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_predictions=lr_model.evaluate(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">87</span><span class=\"ansired\">]: </span>1347.3570328566047\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_predictions.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">88</span><span class=\"ansired\">]: </span>0.02661273286093624\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_predictions.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make predictions on test data \n",
    "test_results=lr_model.evaluate(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+\n          residuals|\n+-------------------+\n-20.727120656520412|\n   78.0331027981332|\n-15.383596278267532|\n -25.12959596818439|\n-19.517276742465015|\n-18.115343387931233|\n -19.32210991674009|\n -21.19758827327675|\n -20.16292013670042|\n 25.650097286580575|\n+-------------------+\nonly showing top 10 rows\n\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view the residual errors based on predictions \n",
    "test_results.residuals.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">91</span><span class=\"ansired\">]: </span>0.024920212367780636\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#coefficient of determination value for model\n",
    "test_results.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">92</span><span class=\"ansired\">]: </span>35.98801713927461\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">93</span><span class=\"ansired\">]: </span>1295.1373776167234\n</div>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "name": "Handling_Time",
  "notebookId": 3.516167528173097E15
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
